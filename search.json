[
  {
    "objectID": "resources/project-card.html",
    "href": "resources/project-card.html",
    "title": "Project Card",
    "section": "",
    "text": "Business View",
    "crumbs": [
      "ML Documentation",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Card</span>"
    ]
  },
  {
    "objectID": "resources/project-card.html#background",
    "href": "resources/project-card.html#background",
    "title": "Project Card",
    "section": "Background",
    "text": "Background\nProvide succinct background to the problem so that the reader can empathize with the problem.\nyour response",
    "crumbs": [
      "ML Documentation",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Card</span>"
    ]
  },
  {
    "objectID": "resources/project-card.html#problem",
    "href": "resources/project-card.html#problem",
    "title": "Project Card",
    "section": "Problem",
    "text": "Problem\nWhat is the problem being solved?\nyour response",
    "crumbs": [
      "ML Documentation",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Card</span>"
    ]
  },
  {
    "objectID": "resources/project-card.html#customer",
    "href": "resources/project-card.html#customer",
    "title": "Project Card",
    "section": "Customer",
    "text": "Customer\nWho it is for? Is that a user or a beneficiary? What is the problem being solved? Who it is for?\nyour response",
    "crumbs": [
      "ML Documentation",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Card</span>"
    ]
  },
  {
    "objectID": "resources/project-card.html#value-proposition",
    "href": "resources/project-card.html#value-proposition",
    "title": "Project Card",
    "section": "Value Proposition",
    "text": "Value Proposition\nWhy it needs to be solved?\nyour response",
    "crumbs": [
      "ML Documentation",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Card</span>"
    ]
  },
  {
    "objectID": "resources/project-card.html#product",
    "href": "resources/project-card.html#product",
    "title": "Project Card",
    "section": "Product",
    "text": "Product\nHow does the solution look like? It is more of the experience, rather how it will be developed.\nyour response",
    "crumbs": [
      "ML Documentation",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Card</span>"
    ]
  },
  {
    "objectID": "resources/project-card.html#objectives",
    "href": "resources/project-card.html#objectives",
    "title": "Project Card",
    "section": "Objectives",
    "text": "Objectives\nBreakdown the product into key (business) objectives that need to be delivered? SMART Goals is useful to frame\nyour response",
    "crumbs": [
      "ML Documentation",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Card</span>"
    ]
  },
  {
    "objectID": "resources/project-card.html#risks-challenges",
    "href": "resources/project-card.html#risks-challenges",
    "title": "Project Card",
    "section": "Risks & Challenges",
    "text": "Risks & Challenges\nWhat are the challenges one can face and ways to overcome?\nyour response",
    "crumbs": [
      "ML Documentation",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Card</span>"
    ]
  },
  {
    "objectID": "resources/project-card.html#task",
    "href": "resources/project-card.html#task",
    "title": "Project Card",
    "section": "Task",
    "text": "Task\nWhat type of prediction problem is this? Link Model Card when sufficient details become available (start small but early)\nyour response",
    "crumbs": [
      "ML Documentation",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Card</span>"
    ]
  },
  {
    "objectID": "resources/project-card.html#metrics",
    "href": "resources/project-card.html#metrics",
    "title": "Project Card",
    "section": "Metrics",
    "text": "Metrics\nHow will the solution be evaluated - What are the ML metrics? What are the business metrics? Link Model Card when sufficient details become available (start small but early)\nyour response",
    "crumbs": [
      "ML Documentation",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Card</span>"
    ]
  },
  {
    "objectID": "resources/project-card.html#evaluation",
    "href": "resources/project-card.html#evaluation",
    "title": "Project Card",
    "section": "Evaluation",
    "text": "Evaluation\nHow will the solution be evaluated (process)? Link Model Card when sufficient details become available (start small but early)\nyour response",
    "crumbs": [
      "ML Documentation",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Card</span>"
    ]
  },
  {
    "objectID": "resources/project-card.html#data",
    "href": "resources/project-card.html#data",
    "title": "Project Card",
    "section": "Data",
    "text": "Data\nWhat type of data is needed? How will it be collected - for training and for continuous improvement? Link Data Cards when sufficient details become available (start small but early)\nyour response",
    "crumbs": [
      "ML Documentation",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Card</span>"
    ]
  },
  {
    "objectID": "resources/project-card.html#plan-roadmap",
    "href": "resources/project-card.html#plan-roadmap",
    "title": "Project Card",
    "section": "Plan/ Roadmap",
    "text": "Plan/ Roadmap\nProvide problem break-up, tentative timelines and deliverables? Use PACT format if SMART is not suitable.\nyour response",
    "crumbs": [
      "ML Documentation",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Card</span>"
    ]
  },
  {
    "objectID": "resources/project-card.html#continuous-improvement",
    "href": "resources/project-card.html#continuous-improvement",
    "title": "Project Card",
    "section": "Continuous Improvement",
    "text": "Continuous Improvement\nHow will the system/model will improve? Provide a plan and means.\nyour response",
    "crumbs": [
      "ML Documentation",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Card</span>"
    ]
  },
  {
    "objectID": "resources/project-card.html#resources",
    "href": "resources/project-card.html#resources",
    "title": "Project Card",
    "section": "Resources",
    "text": "Resources\nWhat resources are needed? Estimate the cost!\nyour response\n\nHuman Resources\nwhat type of team and strength needed?\nyour response\n\n\nCompute Resources\nWhat type of compute resources needed to train and serve?\nyour response",
    "crumbs": [
      "ML Documentation",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Card</span>"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AI-839",
    "section": "",
    "text": "Welcome\nHello Students of AI 839.\nSee the course page for recent information on Lectures, Homeworks, Projects, etc..",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#announcements",
    "href": "index.html#announcements",
    "title": "AI-839",
    "section": "Announcements",
    "text": "Announcements\n\n[12-September-2024] Notes added to W07-L01. W07-L02 is added.\n[05-September-2024] Homework, Minor and Major preoject details added. See HW-05, HW-06, Minor Project, Major Project for details. Course pages updated.\n[05-September-2024] Lecture Page W05-L01, W05-L02, W06-L01, W06-L02 added. Course pages updated.\n[25-August-2024] Lecture Page W03-L02, W04-L01, W04-L02 added. Course pages updated.\n[23-August-2024] HW-03 and HW-04 added.\n[13-August-2024] Lecture Page W03-L01 added. Course pages updated.\n[09-August-2024] Lecture Page W02-L02 added. Course pages updated. HW-02 added\n[06-August-2024] Lecture Page W02-L01 added. Project Card, as a jupyter notebook is added.\n[01-August-2024] Course website up, Lecture Page W01-L01 added. HW-01 added.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "AI-839",
    "section": "Overview",
    "text": "Overview\nPrereqs\n\nExposure and skill in data handling, building models in Python, PyTorch\nExposure and skill in developing code using Python, Git, IDEs like VS Code\nA foundation course in Machine Learning, Deep Learning, Data Modeling, working with (Big) Data\n\nPart-1: Essentials\n\nTopics\n\nbasic principles and MLOps with Open Source Software\nthree assignments\n\nLearning Outcomes: students will be able to\n\ndeploy models with logging, documentation, unit tests, and APIs\nunderstand a conceptual framework to approach MLOps holistically\n\n\nPart-2: Full Stack MLOps\n\nTopics\n\nholistic understanding of ML development, beyond chasing typical performance metrics\none assignment, one mini project and a midterm\n\nLearning Outcomes: students will be able to\n\ndeploy models, observe their performance, make improvements, redeploy them.\n\nensure that the ML pipeline is reproducible.\nincorporate principles from Responsible AI and build ML systems which can consist of many models and tools.\n\n\nPart-3: Intro to LLMOps & Application\n\nTopics\n\npractice, cloud solutions\ncapstone project and presentations\ninvited lectures from Industry\n\nLearning Outcomes: students will be able to\n\nframe, discover, develop, deploy, monitor, improve, re-deploy and maintain an ML Application\napproach the problem holistically, optimize RoI",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "Preface",
    "section": "",
    "text": "Why MLOps?\nFirst and foremost, let us swallow the bitter pill - ML is just a piece of technology like any other to solve a business problem for which somebody needs to pay for. It means that unless people use it and pay for this technology (models), no matter how sophisticated and cool do they sound, they byte the dust like most ML initiatives do.\nNot just that - industry needs and loves determinism, accountability and reliability, which is the antithesis on which ML lives and thrives. MLOps is about addressing this inherent design challenge using a combination of tools and processes. Without which one has to appeal to luck to achieve the desirable. And we know from experience that luck favors the prepared.\nThis collection of notes on MLOps is about that preparation to manage the entire life cycle of an ML product, holistically and comprehensively. That is, for example, the ML system must be performant, explainable, reliable, fair and transparent, responsive and responsible, controllable, cost effective, collaborative, and many others, all which will discussed in detail later in the course.\n\n\nWhy A course on MLOps?\n\nUnderestimation of Technical Debt\n\nDeveloping ML systems involves significant technical complexities that are often underestimated, chiefly arising out of model-centric pedagogy.\nUnlike traditional software development, ML systems require continuous monitoring and iteration of the situated environment\n\nFundamental Differences from Traditional Software Development\n\nTraditional software development assumes a stable environment and known requirements.\nML systems, however, must adapt to changing environments and data, making it impossible to assume a perfect product at deployment.\n\nUncertain Data Environment\n\nAI systems inherently make mistakes and must be designed to handle continuous change and evolution.\nA good model in the lab does not guarantee performance in production, necessitating robust MLOps practices.\nThe deployment and monitoring of ML systems differ fundamentally from traditional software.\nContinuous monitoring and iteration are essential for maintaining model performance and relevance.\n\nOperationalizing ML\n\nOperationalizing ML is crucial for deriving value from AI systems, aligning them with business objectives and real-world applications. This can be a non-trivial activity, if not impossible in some cases.\nThis mindset is essential for understanding the practical aspects of ML deployment and maintenance.\n\n\nA course on MLOps exposes the developer or system designer to this varying challenges in the life cycle of ML (which is perpetual). This is entirely different from how one could solve an ML problem in academic settings where, more often than not, model novelty is incentivized over utility.\n\n\nWhy THIS course on MLOps?\nThere are many great resources available on the web in terms of books, blogs, courses, and tool documentation. Why yet another one? A legit question.\nAt a philosophical level - there are two objectives:\n\nFight the SOTA syndrome:\nLet me explain.\nThanks to chatGPT, it all seems to be about LLMs - building new models, spinning new shiny applications built around those LLMs both open and closed included. These new shiny objects exacerbated an already sick situation - a situation where building models (model.train) showing 0.01% (or even less) increase in accuracy is chased and cherished – all about State-of-the-Art (SOTA).\nWhile we must work hard and smart to move the needle, and it might be satisfying and fulfilling intellectually – unfortunately, it (chasing SOTA) is neither necessary nor sufficient to build reliable systems consisting of at least one ML component. There is lot more one has to consider in building and managing such systems.\nPrioritizes utility over novelty:\nAdmittedly, it is a boring job to be done. However, contrary to the prejudice, in the due course, you will learn that, indeed, solving for utility and overcoming the challenges along the way, is a rewarding journey in itself. Give no cult status to SOTA.\n\nMore concretely, there are some limitations of current MLOps Resources - they are notably:\n\nPractitioner-Focused Literature\n\nMost current MLOps books and resources are aimed at practitioners.\nThese resources focus heavily on tools and practices without structured assessment components.\nOften they anchor on specific framework/tool set.\n\nNeed for Academic Assessment\n\nIn academia, assessments are a crucial part of the learning process.\nThis material should not only teach concepts but also include tests to evaluate understanding.\nTesting is fundamental to learning and ensures that students grasp and can apply MLOps concepts effectively.\n\nComprehensive Curriculum\n\nThe course should cover not only MLOps but also ML system design and the operationalization of models.\nIntegrating these elements ensures a holistic understanding of both the development and deployment aspects of ML projects.\n\n\nSo the chief difference is - most of the resources are meant for practitioners. But this set of notes is meant both for students who can learn the tools and processes, and apply them and also for practitioners (who can learn the principles). But some of the challenges are more systemic and pervasive outlined below:\n\nVariability in MLOps Practices\n\nMLOps processes and practices vary significantly between individuals and organizations.\nUnlike DevOps in the software industry, which has become a mainstream discipline with standardized practices, MLOps is still relatively young.\n\nAbsence of Standard Vocabulary\n\nThere is no non-denominational vocabulary in MLOps, leading to inconsistencies in terminology and practices.\nStandardizing terminology and processes is crucial for effective communication and collaboration within and between organizations.\n\nKnowledge Diffusion and Skill Gap\n\nTraditional academic institutions typically propagate certain ideas and knowledge that get absorbed and amplified in the industry. But the reverse diffusion is often delayed.\nThere can be a lag between what academia offers and what the industry needs, especially in rapidly evolving fields like ML.\nset of templates (of code bases and also of materialized principles) that anybody can use in the Industry which improves the overall employee productivity of the employee\n\nNo Institutionalized MLOps Thinking\n\nInstitutionalizing MLOps in academic programs ensures that students are trained in industry-relevant skills from the start.\nThis approach can help standardize the quality and relevance of ML education, making graduates industry-ready.\nIntroduce a conceptual framework of models, actors and actions involved and a vocabulary to describe the complex ML dev cycle\nProvide a holistic view of ML - going beyond chasing performance metrics\npractice the principle theory-with-code and code-with-theory so that every principle is practicable, and every practice has a principle\ndevelop good (conceptual) models and principles which industry can adopt\n\n\n\n\nBenefits for Students\n\nIndustry Readiness\n\nInstead of learning these skills during internships or on the job, students will acquire them as part of their academic experience.\nThis prepares students to be industry-ready upon graduation, reducing the training burden on employers.\n\nHolistic Approach to Machine Learning\n\nThe course promotes a holistic view of ML, integrating model-centric and data-centric approaches.\nStudents will learn to consider all aspects of ML, including data quality, model robustness, and system integration.\n\nResponsible AI\n\nThe curriculum will cover aspects of responsible AI, ensuring that students are aware of ethical considerations and best practices in AI deployment.\nThis includes understanding biases, fairness, transparency, and accountability in ML models.\n\nComprehensive Skill Set\n\nStudents will gain a broad set of skills, from ML system design to operationalizing models.\nThis comprehensive skill set ensures that graduates are well-prepared to handle the full ML lifecycle in professional settings.\n\n\n\n\nBenefits for Academic Institutions\n\nPioneering Course Offering in India\n\nThis course is likely the first of its kind being offered in India, positioning the institution as a leader in ML education.\nBuilding on existing curricula such as Introduction to Machine Learning, Deep Learning, and Introduction to DevOps.\n\nIndustry-Relevant Education\n\nBy incorporating industry perspectives, including guest lectures and hands-on projects, the course aligns academic training with real-world needs.\nFold industry needs into the curriculum: students often gain experience in developing models and ML system design through internships, and this is transactional by design. This experiential knowledge has to be scaled with backward integration into the curriculum\nThis practitioner-oriented approach ensures students gain practical experience.\n\nEnhanced Employability\n\nAcademic institutions benefit by producing graduates who are immediately employable.\nStrong industry partnerships and placement opportunities can be developed, enhancing the institution’s reputation and appeal to prospective students.\nEquip students with essential skills: in ML lifecycle management, including deployment, monitoring, and automation\nPrepares students for roles such as ML engineer, MLOps engineer, and data scientist with operations expertise\nFamiliarize students with MLOps tools and platforms (e.g., Kubeflow, MLflow, TFX).\nEnhance the overall learning experience: By providing practical, hands-on experience with industry-relevant tools and practices\n\nCater to new market: As there is growing demand for MLOps skills in the industry due to the increasing adoption of AI and ML technologies. Organizations are looking for professionals who can manage the end-to-end ML lifecycle\n\n\n\nBenefits for Industry\n\nReducing Ad-Hoc Practices\n\nThe course aims to reduce the ad-hoc nature of MLOps practices and bring consistency to tooling choices.\nSimilar to how standardized practices in software development (e.g., Java build tools, POM) have streamlined processes, this course seeks to standardize MLOps practices.\n\nBuilding a Talent Pool\n\nThe industry will benefit from a pool of talent that is job-ready, reducing the onboarding time and training costs.\nGraduates will be proficient in MLOps practices, making them productive and profitable employees from the start.\n\nConceptual Frameworks for MLOps\n\nWhile the tools and techniques for implementing MLOps may vary, the course will provide students with robust frameworks and methodologies.\nThese frameworks will help students understand and apply MLOps principles in various contexts and adapt to new technologies as they emerge.\n\n\n\nThis course aims to introduce concepts that will stand the test of time, despite the rapid evolution of tools and techniques.\n\nBy focusing on foundational principles, the course provides a framework for thinking about MLOps that remains relevant as the field evolves.\n\n\n\nConsistency and Knowledge Transfer\n\nStandardized MLOps training ensures that professionals can move between projects with ease, facilitating better knowledge transfer and collaboration.\nThis reduces the time and effort needed to get new hires up to speed on MLOps practices within the organization.\n\n\n\n\nStyle\nContent will be presented in the form of take-away points, rather than main take-aways embedded in long winding paragraphs. Nuances etc will be added in the due course of time or video recordings will be made available.\n\n\nDisclaimer\nThis course is by no means a replacement of any other resources available. Hopefully, the content and views presented complement the current practice of MLOps, readers and students benefit from it.\nopenly,\nThe Saddle Point",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "course.html",
    "href": "course.html",
    "title": "Course",
    "section": "",
    "text": "Syllabus & Schedule",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Course</span>"
    ]
  },
  {
    "objectID": "course.html#discussions",
    "href": "course.html#discussions",
    "title": "Course",
    "section": "Discussions",
    "text": "Discussions\nWe will use WhatsApp group for (informal)discussions and alerts.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Course</span>"
    ]
  },
  {
    "objectID": "course.html#references",
    "href": "course.html#references",
    "title": "Course",
    "section": "References",
    "text": "References\n\n[book] ML Engineering, Andiry Burkov, 2019, LeanPub\n[book] Effective Data Science Infrastructure, Vile Tuulos, 2023, Manning\n[book] ML System Design, Chip Huyen, 2023, O’Reilly\n[course] CS329S @ Stanford: ML Systems Design, Chip Huyen, 2022\n[course] MLOps, Chip Huyen, 2024",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Course</span>"
    ]
  },
  {
    "objectID": "course.html#grading",
    "href": "course.html#grading",
    "title": "Course",
    "section": "Grading",
    "text": "Grading\n\n40%: Six assignments\n15%: Midterm mini project\n20%: In-class midterm\n25%: Capstone project",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Course</span>"
    ]
  },
  {
    "objectID": "course.html#policies",
    "href": "course.html#policies",
    "title": "Course",
    "section": "Policies",
    "text": "Policies\n\nLate Submissions:  All deadlines are due at on the date and time indicated on the course page. The penalties for late submission are as follows:\n\nLate submissions not allowed (incur a zero) - except with prior approval or in valid exceptional cases\n\nMake-up Exam/Submission Policy: As per institute policy\nCitation Policy for Papers: Always mention the source, give full attribution and credits to citations, and as per institute policy\nAcademic Dishonesty/Plagiarism: As per institute policy\nAccommodation of Divyangs: As per institute policy\n\nSoma S Dhavala\nCourse Instructor",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Course</span>"
    ]
  },
  {
    "objectID": "lectures/w01-l01.html",
    "href": "lectures/w01-l01.html",
    "title": "01A: Grounding MLOps",
    "section": "",
    "text": "Materials:\nDate: Thursday, 01-Aug-2024.",
    "crumbs": [
      "Lectures[ML Engg.]",
      "01A: Grounding MLOps"
    ]
  },
  {
    "objectID": "lectures/w01-l01.html#materials",
    "href": "lectures/w01-l01.html#materials",
    "title": "01A: Grounding MLOps",
    "section": "",
    "text": "Pre-work:\n\nRefresh ML foundations.\nRead “The 100 page ML book” by Andiry Burkov. Chapters accessible here\n\n\n\nIn-Class\n\nWe will go over W01-L01-Deck based on this from Chip Huyen’s CS329S: ML System Design Fall’22 @ Stanford\nSoftware 1.0 vs Software 2.0. See this Figure\n\n\n\nPost-class:\n\nRead the motivation for MLOps from this blog.\nML Life Cycle CRISP-ML9(Q): blog, paper\nCh1-2 of AB, Ch1 of VT, Ch1 of CH1",
    "crumbs": [
      "Lectures[ML Engg.]",
      "01A: Grounding MLOps"
    ]
  },
  {
    "objectID": "lectures/w02-l01.html",
    "href": "lectures/w02-l01.html",
    "title": "02A: Grounding MLOps",
    "section": "",
    "text": "Materials:\nDate: Tuesday, 06-Aug-2024.",
    "crumbs": [
      "Lectures[ML Engg.]",
      "02A: Grounding MLOps"
    ]
  },
  {
    "objectID": "lectures/w02-l01.html#materials",
    "href": "lectures/w02-l01.html#materials",
    "title": "02A: Grounding MLOps",
    "section": "",
    "text": "Pre-work:\n\nRead the motivation for MLOps from this blog.\nReview W01-L01-Deck based on this from Chip Huyen’s CS329S: ML System Design Fall’22 @ Stanford\n\n\n\nIn-Class\n\nML Production Myths W01-L01-Deck\nSoftware 1.0 vs Software 2.0. See this W01-L02-Deck\nMLOps Motivation\nAI/ML Canvas from ml-ops.org Phase-Zero, Gokul Mohandas’s Project Canvas\n\n\n\nPost-class:\n\n[blog] A simple Tool to Start Making Decisions with the Help of AI\n[book, optional] Prediction Machines: The Simple Economics of AI, Ajay Agrawal, Joshua Gans, Avi Goldfare\n[blog] Why Metaflow\n[blog] Building and Managing Data Science Pipelines with Kedro from neptune.ai",
    "crumbs": [
      "Lectures[ML Engg.]",
      "02A: Grounding MLOps"
    ]
  },
  {
    "objectID": "lectures/w02-l02.html",
    "href": "lectures/w02-l02.html",
    "title": "02B: Discover",
    "section": "",
    "text": "Materials:\nDate: Friday, 09-Aug-2024.",
    "crumbs": [
      "Lectures[ML Engg.]",
      "02B: Discover"
    ]
  },
  {
    "objectID": "lectures/w02-l02.html#materials",
    "href": "lectures/w02-l02.html#materials",
    "title": "02B: Discover",
    "section": "",
    "text": "Pre-work:\n\nMLOps Motivation\nAI/ML Canvas from ml-ops.org Phase-Zero, Gokul Mohandas’s Project Canvas\nML Life Cycle CRISP-ML9(Q): blog, paper\n\n\n\nIn-Class\n\nDesign Thinking from Stanford Design School. It is a useful ideation tool to formulate a problem and develop a solution.\nProject Cards a Jupyter Notebook template for Project Cards. It touches the aspects of data-driven documentation and maintaining single source of truth for any document containing data. A WIP document on Data-driven ML Documentation. This also forms an important aspect of Transparency\nW02-L02 deck\n\n\n\nPost-class:\n\n[blog] MLOps Tech Stack\n[canvas] MLOps Canvas\n[site] Data Nutrition Project, CLeAR Framework paper, Data Nutrition Label paper\n[blog] 10 Dimensions of making data science work by Goda Ramkumar\n\nDimension 1: Expectation\nDimension 2: Strategy\nDimension 3: Roles\nDimension 4: Collaboration\nDimension 5: Culture\nDimension 6: Discovery and Access\nDimension 7: Platforms\nDimension 8: Scalability\nDimension 9: Methodology and Practices\nDimension 10: Quality and Governance [tbd]",
    "crumbs": [
      "Lectures[ML Engg.]",
      "02B: Discover"
    ]
  },
  {
    "objectID": "lectures/w03-l01.html",
    "href": "lectures/w03-l01.html",
    "title": "03A: Models for Modeling",
    "section": "",
    "text": "Materials:\nDate: Tuesday, 13-Aug-2024.",
    "crumbs": [
      "Lectures[ML Engg.]",
      "03A: Models for Modeling"
    ]
  },
  {
    "objectID": "lectures/w03-l01.html#materials",
    "href": "lectures/w03-l01.html#materials",
    "title": "03A: Models for Modeling",
    "section": "",
    "text": "Pre-work:\n\nMLOps Platforms some goals\nConvention over Configuration\nData Flow Paradigm\n\n\n\nIn-Class\n\nML model pipelines are Directed Acyclic Graphs. ML Workbench developed at EkStep was one of the early implementations of this primitive.\nW03-L01 deck\nKedro Tutorial: Part-0 and Part-1. Hands-on and commentary\n\n\n\nPost-class:\n\n[blog]Shreya Shankar on some MLOps Principles\n[git template] MLOps cookie cutter template for another opinionated project structure",
    "crumbs": [
      "Lectures[ML Engg.]",
      "03A: Models for Modeling"
    ]
  },
  {
    "objectID": "lectures/w03-l02.html",
    "href": "lectures/w03-l02.html",
    "title": "03B: DevOps for ML",
    "section": "",
    "text": "Materials:\nDate: Friday, 16-Aug-2024.",
    "crumbs": [
      "Lectures[ML Engg.]",
      "03B: DevOps for ML"
    ]
  },
  {
    "objectID": "lectures/w03-l02.html#materials",
    "href": "lectures/w03-l02.html#materials",
    "title": "03B: DevOps for ML",
    "section": "",
    "text": "Pre-work:\n\nMLOps Platforms some goals\nConvention over Configuration\nData Flow Paradigm\n\n\n\nIn-Class\n\nGit, Docker, Docker Compose, MinIO Object Store\nGit Actions, CI/CD\nKedro Tutorial Hands-on: Kedro pipeline execusion via airflow\nW03-L02\n\n\n\nPost-class:\n\n[Doc] Kedro-Docker Deploy Kedro pipline using Docker via Kedro-Docker plug-in\n[blog]Git Flow a mental model of how to work with Git, branching process, and feature development\n[video] Git actions for Metrics tracking\nFew additional resources on Git, Git Actions, Docker in the Tutorials chapter.\n[code] Data Science Life Cycle Process a git flow like Issue tracker and branching process more suitable for Data Science Projects.",
    "crumbs": [
      "Lectures[ML Engg.]",
      "03B: DevOps for ML"
    ]
  },
  {
    "objectID": "lectures/w04-l01.html",
    "href": "lectures/w04-l01.html",
    "title": "04A: Develop",
    "section": "",
    "text": "Materials:\nDate: Tuesday, 20-Aug-2024.",
    "crumbs": [
      "Lectures[ML Engg.]",
      "04A: Develop"
    ]
  },
  {
    "objectID": "lectures/w04-l01.html#materials",
    "href": "lectures/w04-l01.html#materials",
    "title": "04A: Develop",
    "section": "",
    "text": "Pre-work:\n\nKedro Tutorial Part-0, Part-1.\n\n\n\nIn-Class\n\nA framework to think about metrics. For example, by focussing on who the the solution is for, metrics such as affordability, availability, accessibility, become definable.\nWorking with notebooks in Kedro (Kedro context magic)\nAPI documentation with quartodoc based on quarto, an alternative to sphinx\nTesting with PyTest.\nLinting and formating with ruff.\nW04-L01 Metrics slides #35-38 (more on metrics section)\n\n\n\nPost-class:\n\n[Deck] W04-L01. Review the material.\n[Docs] Kedro-Docker work with notebooks inside Kedro.\n[Docs] quartodoc Turn your docstrings into documentation with quartodoc. Plus any other content (notebooks, qmd files) can be rendered using quarto of course.\n[Docs] PyTest in Kedro\n[Docs] [ruff in Kedro](See Kedro docs",
    "crumbs": [
      "Lectures[ML Engg.]",
      "04A: Develop"
    ]
  },
  {
    "objectID": "lectures/w04-l02.html",
    "href": "lectures/w04-l02.html",
    "title": "04B: Monitor (Data)",
    "section": "",
    "text": "Materials:\nDate: Friday, 23-Aug-2024.",
    "crumbs": [
      "Lectures[ML Engg.]",
      "04B: Monitor (Data)"
    ]
  },
  {
    "objectID": "lectures/w04-l02.html#materials",
    "href": "lectures/w04-l02.html#materials",
    "title": "04B: Monitor (Data)",
    "section": "",
    "text": "Pre-work:\n\nSee how logistic regression test cases were written in sklearn. In particular, see how the test was prepared which makes it possible to test the fitted coeffcients analytically.\nSee how Peceptron test cases were written on Iris data.\n\n\n\nIn-Class\n\nQA for Data. It is all about asserting the statistical properties of the data.\nDiscussion on extracting statistical quality features into a table for any modality (tabular, image, speech, text)\nTesting columnar data with explicit conditions. Great Expectations defines them as expectations and validates them given a new data. For example, an expectation can be\n\na column can have at most 5% missing values\nthe range of the columns can be between [-2,10]\n\nTesting columnar data with implicit conditions. One dataset will be compared against a reference dataset. Evidently comes with many tests for reporting, model comparison, data drift detection. For example, we can compare whether or not the label distribution is same between the Train set and the Test set. A question for all readers - how often have you “actually” ran any statistical test to see if the Train and Test set are actually similar in distribution. My guess, less than 10% would have done it. The remaining would have called sklearn’s train_test_split function :)\n\n\n\nPost-class:\n\n[Blog] ETL testing with Great Expectations\n[Docs] Great Expectations Documentation. Please note that with version 1.0 released, even the examples from its repo are not working. Read them to understand what are typical tests on columnar data look like.\n[Notebooks] Evidently examples. Browse through and run how Evidently automates many test cases. Also see community examples here",
    "crumbs": [
      "Lectures[ML Engg.]",
      "04B: Monitor (Data)"
    ]
  },
  {
    "objectID": "lectures/w05-l01.html",
    "href": "lectures/w05-l01.html",
    "title": "05A: Monitor (Models)",
    "section": "",
    "text": "Materials:\nDate: Tuesday, 27-Aug-2024.",
    "crumbs": [
      "Lectures[ML Engg.]",
      "05A: Monitor (Models)"
    ]
  },
  {
    "objectID": "lectures/w05-l01.html#materials",
    "href": "lectures/w05-l01.html#materials",
    "title": "05A: Monitor (Models)",
    "section": "",
    "text": "Pre-work:\n\nReview Diagnostics of ML Systems from CS329s\n\n\n\nIn-Class\n\nRecap tests for monitoring data quality in the ETL stage\nUnderstand different types of drifts (label, covariate, concept) and ways to detect them that may be important in the model development stage\nCollecting data in the training stage - see Training Data, from CS329s\nData Programming - programmatically create labels. snorkel introduced these ideas first in the NLP space.\nSynthetic Data Generation using LLMs is a very promising and emerging application LLMs. For example, we can create labels of a piece of text, by prompting LLMs.\n\n\n\nPost-class:\n\nReview Training Data, from CS329s\nReview Feature Engineering, from CS329s",
    "crumbs": [
      "Lectures[ML Engg.]",
      "05A: Monitor (Models)"
    ]
  },
  {
    "objectID": "lectures/w05-l02.html",
    "href": "lectures/w05-l02.html",
    "title": "05B: Deployment",
    "section": "",
    "text": "Materials:\nDate: Friday, 30-Aug-2024.",
    "crumbs": [
      "Lectures[ML Engg.]",
      "05B: Deployment"
    ]
  },
  {
    "objectID": "lectures/w05-l02.html#materials",
    "href": "lectures/w05-l02.html#materials",
    "title": "05B: Deployment",
    "section": "",
    "text": "Pre-work:\n\nmlflow docs\nkedro-mlflow tutorial\n\n\n\nIn-Class\n\nStream vs Batch Deployment\nHands-on Deploying models with mlflow on local server\n\n\n\nPost-class:\n\nReview Model Deployment from CS329s. [Note: Dr. Srinivas Rana will cover Model Compression, Quantization etc in his talk on Deployment on Edge]\nReview Monitoring and Continual Learning from CS329s\nReview Model Deployment chapter of ML Engineering book",
    "crumbs": [
      "Lectures[ML Engg.]",
      "05B: Deployment"
    ]
  },
  {
    "objectID": "lectures/w06-l01.html",
    "href": "lectures/w06-l01.html",
    "title": "06A: Evaluate",
    "section": "",
    "text": "Materials:\nDate: Tuesday, 03-Sep-2024",
    "crumbs": [
      "Lectures[ML Engg.]",
      "06A: Evaluate"
    ]
  },
  {
    "objectID": "lectures/w06-l01.html#materials",
    "href": "lectures/w06-l01.html#materials",
    "title": "06A: Evaluate",
    "section": "",
    "text": "Pre-work:\n\nReview Kolmogorov-Smirnov Test as a way to measure divergence between two continous, univariate distributions\nReview KL Divergence as a way to measure divergence between two arbitrary (continuous/discrete and univariate/multivariate) distributions\n\n\n\nIn-Class\n\nMotivation for why we need Design of Experiments and Hypothesis Testing. Where do they appear in the ML Life Cycle.\nLesson 1 - quick intro to DoE, scientific objectives, basic principles of DoEs, steps for planning, conducting, and analyzing an experiment.\nLesson 2 - a simple comparative experiment. The name A/B Testing, perhaps, comes from testing difference between two groups A and B, which is a simple comparative experiment. How to define a business problem as a hypothesis test, collect data, perform the test, draw conclusion are demonstrated. How to calculate the sample size, probably the most important question that gets asked, is also explained in this simple case.\n\n\n\nPost-Class\n\nReview Model Selection from CS329s\nReview Model Evaluation chapter of ML Engineering book\n\n\n\nReferences:\n\n[book] A/B Testing. This book gives a non technical introduction to A/B testing and how they get applied in the e-commerce, website UX optimization, running marketing campaigns. In the appendix, many scenarios of A/B testing are covered.\n[book] Design and Analysis of Experiments. This is a classic in DoE. Lessons 1-2 are necessary.\n[course]STAt 503 Design of Experiments - online course at UPenn\n[course]STAt 514 Design of Experiments - course at Purdue (stats oriented). Chapters 1-4 are needed.\n[book] Statistical Design. This is another classic from George Casella, a celebrated science author.\n\n\n\nNotes\n\nQA for Data discussed earlier is a specific case of A/B testing. CS folks call it A/B testing, but they are all different types of hypothesis tests.\nHypothesis Tests are tools for testing aspects of data. In the ML context, they can be used for testing\n\ndata drift (concept drift, covariate drift, label drift)\ndata quality (implicit and explicit)\nmodel performance\n\nOn Model Testing/ Comparison as a Hypothesis\n\nIs the “alternate” model better than the “baseline” model? Often, ML folks report performance metrics on Test split for all the models and pick the model with the highest performance. It is not uncommon to claim SOTA even when the performance difference is in 1/100ths of decimal places and replication is almost absent :). We do not even know if this difference is due to just chance (randomness in the data) alone. A rigorous (and perhaps, the right) approach would have been to formulate this as a hypothesis test, design an experiment, collect evidence and then conclude which model is better (and is statistically significant). Note that, statistical significance does not mean practical significance, which is often the case with most SOTA claims :). Another classical example to drive this point home is the (in)famous Netflix prize. The top performing model in the million dollar competition never made it to deployment. Find out why.\n\nHyperparameter Tuning and AutoML is a DoE in disguise\n\nthe experimental factors are, for example, the architecture, optimizer, learning rate, batch size, among others. The response variable is the performance. Techniques like Bayesian Optimization, and many techniques used in AutoML are indeed Sequential DoEs (you explore the search space sequentially by looking at the past exploration data). Grid-search, the naïve approach to hyperparameter tuning, can be seen as an implementation of a full-factorial DoE.\nWill ideas from DoE such as a Blocking lead to better search strategies? Can we find out if the learning rate and the optimizer (eg. Adam vs AdamW) interact with each other? The experiments to address these questions are often referred to as Ablation Studies in the ML community. So, by learning the principles of DoEs, we will be able to design (data and compute) efficient ablation studies.\n\nHow to plan and collect data for an ML problem?\n\nin the model-centric ML development, which most, if not all students, start their ML training in, will work on a data given to them in a platter. It is rare for an (undergraduate) student to have taken part in the data collection planning exercise. But once they are thrown into industry, they have to confront a very difficult question? We will address these questions in the next session.",
    "crumbs": [
      "Lectures[ML Engg.]",
      "06A: Evaluate"
    ]
  },
  {
    "objectID": "lectures/w06-l02.html",
    "href": "lectures/w06-l02.html",
    "title": "06B: Govern",
    "section": "",
    "text": "Materials:\nDate: Friday, 06-Sep-2024",
    "crumbs": [
      "Lectures[ML Engg.]",
      "06B: Govern"
    ]
  },
  {
    "objectID": "lectures/w06-l02.html#materials",
    "href": "lectures/w06-l02.html#materials",
    "title": "06B: Govern",
    "section": "",
    "text": "Pre-work:\n\nCRUD- create, read, update and delete operations are the primitives we need to make the IT systems work.\nACID- against those CRUD operations, a database needs to support atomicity, consistency, isolation, durability to guarantee data validity, despite errors, and failures.\nSet Theory - is the mathematical basis for relational databases (RDBMS)\nGraph Theory - is probably the mathematical basis for graph databases like Neo4j. NoSQL databases might lie somewhere in the middle. Since Graphs are more general mathematical objects, we might think for now that, it is important to understand graph theory to design graph databases.\nD4M- Dynamic Distributed Dimensional Data Model - is a computer programming model that combines the advantages of distinct data processing technologies (sparse linear algebra, associate arrays, fuzzy algebra, distributed arrays, triple-store/NoSQL databases) to improve search, retrieval and analysis of data.\n\n\n\nIn-Class\n\nThe CRUD operations on Model Predictions\nThe R4 (Read, Replay, Recall, Replace) Framework for Level 5 Data Governance.\n\n\n\nPost-class\n\n[book] An Elementary Introduction to Statistical Learning Theory. See Chapter 8 for how the theory of VC Dimension can be applied to get an idea on the sample size.\n[site] GDPR - in particular focussing on privacy, and right to be forgotten (or right to erasure)\n[wiki] Self Driving Cars read the six levels (Level 0 to Level 5) of autonomy. Level 5 is fully autonomous self driving car.\n\n\n\nAdditional Reading (optional)\n\n[book] Mathematics of Big Data\n\n\n\nNotes\nAt at the heart of it, all of IT applications, eventually perform CRUD operations. Depending on the type of transaction, purpose, and SLAs, the choice of the database (SQL vs NoSQL), Batch-vs-Stream, OLAP vs OLTP, among others will be made. A whole lof of mathematical theories (set theory, graph theory, associative arrays, multi linear algebra, etc) are necessary to build the technologies.\nIn yet another simplified view, ML systems are glorified auto-fills. Take out the fundamental uncertainty in the data produced by ML systems, they are akin to IT systems. Therefore, same infrastructure and engineering process to support CRUD operations on ML produced data can be applied. But is that sufficient?\nOne major difference is - ML systems have memory. Therefore, deleting a record, does not lead to deleting its influence in downstream tasks right? A simple example will drive home the point. Imagine a model is trained on a dataset. There are some records which are leaking private data. What would deleting mean here. Obviously, one can delete those records from the training set. But one must also edit the models to get rid of their influence. Think one step further. What if, these marked records are very close the some other records. Deleting and retraining on the updated training data does not mean that their residual effect is removed. So, it is not only complex but also complicated. When ML models are cascaded and appear in a sequence of data events – controlling their exposure and affects requires a very good control on the downstream consumer applications.\nLet us try to re-interpret CRUD in the ML context. It leads us to the R4 {Read, Repeat, Recall, Replace} framework.\n\nRead\nOn demand, read (or retrieve) the prediction made by an ML model. This require maintaining proper metadata to retrive the records. For example, if the input (request to the API) is available, as is, and the output (response) of the API is logged into a persistent object store, this is possible.\n\n\nRepeat\nOn demand, repeat (or reproduce) the prediction made by an ML model. This require maintaining the three things for the sake reproducibility: 1. The data (inputs) and model artifacts 2. The code which loads the model, and scores on the given data and 3. The runtime to execute the code & data.\n\n\nRecall\nOn demand, recall the prediction made by an ML model. Depending on the “definition” of recall, the implementation and engineering complexity varies. In the simplest case which itself can be very complex, all past instances where the model scored, the predictions have to be replaced with say nulls. However, this implementation is not yet actionable by itself. The downstream consumer for example, can be notified of the recall, and take an appropriate action. The downstream consumer must have the Read ability in the R4 framework. In the best case scenario, the downstream consumer updates the decision (say with human-in-the-loop) or has another back-off strategy implemented already.\nIn the highest form of recall, all downstream consumers of the predictions are notified and decisions are propagaed in the entire chain of events.\n\n\nReplace\nOn demand, replace the prediction made by an ML model with another prediction (after correcting). Once the record is updated, in the most sophisticated case, all downstream consumers propagate the change downstream.\nLike in self-driving cars, the Level of Automation can be categorized.\n\n\n** R4 Levels**\n\n\n\n\n\n\n\n\nLevel\nName\nSupported Actions\nPlatform Features\n\n\n\n\nL0\nR0\nFire and Forget  Zero Traceability\nModel APIs\n\n\nL1\nR1  Read\nTrace past decisions\nObservability\n\n\nL2\nR2  Read & Repeat\nTrace and Recreate past decisions\nObservability +  Versioning + CD\n\n\nL3\nR3  Read, Repeat, Recall\nTrace, Recreate, and Nullify past decisions\nObservability  Versioning + CD +  CImp\n\n\nL4\nR3-N  Read, Repeat, Recall with Notify\nTrace, Recreate, Nullify, and Notify past decisions\nObservability  Versioning + CD +  CImp +  Event-IO\n\n\nL5\nR4  Read, Repeat, Recall, Replace\nTrace, Recreate, and Rescore past decisions\nObservability  Versioning + CD +  CImp\n\n\nL6\nR4-N  Read, Repeat, Recall, Replace with Notify\nTrace, Recreate, Rescore, and Notify past decisions\nObservability  Versioning + CD +  CImp +  Event-IO\n\n\n\n\nAre these four primitive operations sufficient to support any model governance? 1. Can a model be debiased? 2. Can right to be erased to enabled? 3. Many more.\nAs one can see, from an engineering stand point, certain platform features and functionalities are needed.\n\nRead: An observation platform to log data, with CRUD apis on the logs - can enable reading any past decisions (model predictions)\nRepeat: Versioning of data, models, code, and run time with Continuous Deployment (CD) can ensure reproducibility.\nRecall: A Continuous Improvement (CImp) capability that can take any alternate model (which can produce Nulls), deploy it, and rescore on any past data, can enable recalling past decisions.\nReplace: Same capability as Recall except that, a new model would correct past decisions.\nNotify: One has to make the memory-less system into a system with memory, in in async fashion. Implementing a pub-sub model, where downstream consumer subcribes to the R4 topics published by the prediction maker service. For example, a model service published a recall topic with all the decisions that need to be annulled in someway. The subscribers of this topic will take action however they deem fit. They can even publish another recall event and all of its subscribers can act accordingly. This way, a recall pipeline is created based on event triggers.",
    "crumbs": [
      "Lectures[ML Engg.]",
      "06B: Govern"
    ]
  },
  {
    "objectID": "lectures/w07-l01.html",
    "href": "lectures/w07-l01.html",
    "title": "07A: Scaling Laws",
    "section": "",
    "text": "Materials:\nDate: Tuesday, 10-Sep-2024",
    "crumbs": [
      "Lectures[ML Science]",
      "07A: Scaling Laws"
    ]
  },
  {
    "objectID": "lectures/w07-l01.html#materials",
    "href": "lectures/w07-l01.html#materials",
    "title": "07A: Scaling Laws",
    "section": "",
    "text": "Pre-work:\n\nLesson 2 of Stat503 on planning a simple comparative experiment\n\n\n\nIn-Class\n\nReview sample size calculation in t-test. How sample size is a function of data quality, confidence, and tolerance for errors in the conclusions. Except in simple cases, getting a good sense of the “how much data” is a hard, rather very hard question. But we can try.\nMyth Buster - more data is better\n\nit is like a tautology. this notion never gets challenged. More data does not lead to better RoI. In fact, more of the same can never improve performance beyond a point or in some specific cases, it is impossible.\nlaw of diminishing returns. collecting more data can not only be expensive but can saturate, reaching a plateau. In fact, collecting data to understand where this plateau and what is the ceiling is, is an interesting problem in itself. A well designed experiment can address this question.\n\nCan we estimate the sample size needed?\n\nback of the envelope calculations based on some idea about the data, based on two-sample sample size calculations\nfrom PAC theory bounds (Chapter 8 of An Elementary Introduction to Statistical Learning Theory)\nempirical scaling laws\n\n\n\n\nPost-class\n\n[book] An Elementary Introduction to Statistical Learning Theory. See Chapter 8 for how the theory of VC Dimension can be applied to get an idea on the sample size.\n[youtube] PAC Learning Ali Ghosi Lec 19 PAC Learning, STAT 441/841 Statistical Learning, University of Waterloo\n[paper] Training Compute-Optimal LLMs\n\n\n\nAdditional Reading (optional)\n\n[book-online] Linear Models - first regression course for EPFL mathematicians.\n[paper] Approximation Rates and VC-Dimension Bounds for (P)ReLU MLP Mixture of Experts\n[book] Understanding Machine Learning: From Theory to Algorithms - a classic from Shai Shalev-Shwartz and Shai Ben-David. Part-1. youtube playlist\n[paper] Scaling Laws for Neural Language Models\n[paper] An empirical study of scaling laws for transfer learning\n[paper] Scaling laws for Individual data points\n[paper] Scaling laws in Linear Regression",
    "crumbs": [
      "Lectures[ML Science]",
      "07A: Scaling Laws"
    ]
  },
  {
    "objectID": "lectures/w07-l01.html#notes",
    "href": "lectures/w07-l01.html#notes",
    "title": "07A: Scaling Laws",
    "section": "Notes",
    "text": "Notes\nWhen embarking on a problem, the first set of questions we are asked are:\n\nWhat type/kind of data is needed or useful for the problem at hand?\nHow many samples do you need? A new name for this is scaling laws :)\nHow hard the problem is - are all examples will be equally important?\n\nLet us briefly discuss the first question about “what kind”.\n\nWhat to collect?\nWe discussed ideation tools like Design Thinking, Human-centered Design to engage with the problem (to define what to solve). In the Project Canvas, we also discussed what is the ML Task, and what kind of data is needed to solve the business problem.\nFor example, suppose the problem is, say, assessing the quality of food grains. The business problem is to objectively assess the grain quality and decide a fair price linked to the quality. ML problem is to grade the grains, identify any foreign materia/contaminations, may be based on the morphology of the grains. What kind of data is needed here? We can treat this problem as a combination of segmentation, object detection, object classification, morphology analysis, and granulometry. Data must support these tasks. See the blogs Image Annotation Definition, Usecases and Types, Best practices for successful image annotation, Data Annotation - A beginner’s guide to delve into annotating images further.\nArguably, annotating images for segmentation at grain level is very time consuming but can potentially be very useful. Treating this problem as a grading problem (accept/ reject) as opposed to provide quality assessment at grain and sample level is relatively easy in terms of annotations. But with accept/reject the lot binary labels at image level one can not pivot to a different ML problem formulation if the solution is proved not useful. This is an inherent design challenge. Getting as many details as possible is desirable but many not be practical. But thankfully, with foundation models like SAM, some of this leg work can be automated. See this paper on sample selection for efficient image annotation. Most modern annotation studios like Labeller offer some support to speed-up the process.\nWhile the above labels support the primary ML task, say the segmentation task, for example, they may not be sufficient to offer SLAs or Model monitoring (battling the unknown unknowns) that we discussed in Model Monitoring. Suppose, for the same problem, the model is deployed in two grain collection sites. Training data was collected from the the first site (for rice grains), and model is being is used in the second site. It so happens that the second site is using the model for “wheat” instead of “rice.” Obviously, model will perform poorly. This may not have been anticipated ahead of time. Only way to detect this is to first log the data where the model is being used, and observe the performance by site. That means, where the model is being in this case and any other contextual data is necessary to monitor, diagnose, and improve models over time. This is all but one of the dimensions to think about. Generally speaking, thinking about dimensions is the topic of knowledge representation and knowledge engineering. Dimensional Modeling](https://www.ibm.com/docs/en/ida/9.1.2?topic=modeling-dimensional) is a framework to adopt to collect data that can address the concerns of different stakeholders besides the ML and Data Scientists such as a Software Engineer, QA, Architect, Product Manager, Site Manager etc. IBM’s book on Dimensional Modeling: In a Business Intelligence Environment is a definitive guide on this topic. A knowledge Engineering Primer is perhaps a lighter compared to the book.\nNow let us move to the other question. How many?\n\n\nHow many?\nThis question gets very complicated in no time. And there can be more than one way to develop a heuristic to arrive at an approximate answer. The type of response and approach also depends on the scenario.\n\nExact problem was solved before or data is available which can be used as-is [best case]. Take the data, build a model, and deploy and start using. Such occurrences can be rare but can happen. For example, people counting of a given demographic from traffic camera feeds.\nSimilar problem solved, except for differences in domain. Take a model trained on that data, collect data on the target domain. Start improving the model over a period of time. Use transfer learning/ supervised fine-tuning methods. If data exists (in literature or on huggingace/kaggle for eg) for that problem, plot accuracy vs sample size and pick a target that gives desired performance. Iterate over it. Do not collect all data at once.\nUnlabelled data available in large quantities. In such a case, run a pre-trained model or develop self-supervised learning tasks, get very good representations and try to label those that are easy and/or important for the performance. See cords and other active learning methods.\nData exists and similar problem is not solved before [worst case]. Then, one has to design an experiment, provide some inputs (design considerations or operating environment) about the problem, get a ball park sense of the sample size (for the sake of budget, resource and project planning), run a pilot data collection exercise, refine the strategy and iterate. Below, we develop some heuristics to estimate the sample size.\n\n\nA Statistical Approach\nLet us simplify and consider a regression problem. We are trying to learn a function \\(f: [0,1] \\rightarrow R\\). Imagine you are fitting a decision tree to approximate this function. A decision tree partitions the input space, and in each of the partitions certain statistics like mean and quintiles are computed. For a given instance, the prediction is given by, for example, the mean of all responses of the examples belonging to that partition. So, we can divide or cluster or partition the training data into \\(K\\) subsets and compute some statistic in each these subsets. If unlablled data is available, running a clustering algorithms will give an idea about \\(K\\). If we assume that the labels (responses) of the k-th partition denoted by \\(y_k\\) follow \\(N(\\mu_k, \\sigma^2)\\), we can estimate the \\((1-\\alpha)\\) level prediction interval (PI) for \\(\\mu_k\\) as \\[\\bar{y}_k + z_{1-\\frac{\\alpha}{2}}\\sigma \\sqrt{1+\\frac{1}{N}}\\]\nwhere \\(\\bar{y}_k\\) is the sample mean, \\(N\\) is the sample size, \\(\\sigma^2\\) is the noise variance, \\(\\alpha\\) controls the confidence level (or type-1 error of the corresponding hypothesis test). So the “design inputs” needed to solicit a sample size are: \\(\\alpha\\), \\(\\sigma^2\\). Sometimes, the precision needed for the estimate can be asserted in terms of the width of the interval (PIW). In this case, PIW is given as \\(PIW = 2z_{1-\\frac{\\alpha}{2}}\\sigma \\sqrt{1+\\frac{1}{N}}\\). Now, we can express sample size as a function of \\(PIW\\), \\(\\alpha\\) as: \\(N = \\left(  (\\frac{PIW}{2z_{1-\\frac{\\alpha}{2}}})^{2}-1 \\right)^{-1}\\). If there are \\(K\\) partitions, we need to estimate that many \\(\\mu_k\\)s. So, the total sample size will be \\(NK\\) assuming all partitions have same variance. If not, is is not hard to update the formula. In someways, the model complexity is captured by \\(K\\). In general, one does not know these numbers in advance and has to make an educated guess based on domain knowledge and refine the design inputs as the data collection drive is set in motion.\nWhat about the classification problem?\nAssume it is a binary classification problem. Approach is still identical. Even in the classification setting, estimating the mean and taking argmax to predict the label of the partition is still useful and applicable except that the \\(PI\\) formula needs to be updated. For other types of Tasks, suitable estimate of the target varaible has to be chosen, and derive its PI, and use it to get an estimate of the sample size.\n\n\nAn ML Approach\nCan we relax the assumptions and yet come-up with some estimates for \\(N\\)?\nWe can invoke PAC theory. Suppose \\(\\epsilon\\) is the tolerable error (that the test error should not exceed), \\(\\delta\\) is the confidence in the learning algorithm that test error can exceed \\(\\epsilon\\) by no more than \\(\\delta\\) fraction of times and \\(V\\) is the VC-dimensionof the function class. The following bounds from PAC theory can guide us:\n\\[\\frac{1}{\\epsilon}\\left(4\\log(\\frac{2}{\\delta}) + 8V\\log(\\frac{13}{\\epsilon}) \\right) \\le N \\le \\frac{64}{\\epsilon^2}\\left(2V \\log(\\frac{V}{\\epsilon}) + \\log(\\frac{4}{\\delta}) \\right)  \\]\nIn particular, if one uses an MLP, following is a lower bound on the VC-dimension \\[V \\le 2(d+1)s \\log(2.718s)\\] where \\(d\\) is the number of features, \\(s\\) is the number of perceptrons in the network. So, in effect, given an MLP architecture (with \\(s\\) number of perceptrons), confidence(\\(\\delta\\)), permissible error (\\(\\epsilon\\)), input features (\\(d\\)), one can get an idea of the sample size.\nCaution: These bounds can be very vacuous and many not be uniformly tight across the range of the inputs. For example, consider the inequality \\(x^2 \\ge x, \\forall x \\ge 1\\). The gap \\(|x^2-x|\\) grows unboundedly and gets worse as \\(x\\) increases. So, always play with several input values and pick a sensible one. Do not just believe that they will work out of the box. No magics here.\n\n\nA DL Approach\nFor tabular problems, one can use either of the above methods to get a sensible estimate. But for speech, vision, and language datasets, it is both complicated and simple at the same time. Simple in the sense that, one could take pre-trained foundation models and work with their representations and technically use PAC-theory-based heuristics. But the latent dimensions could be extremely large. So, if unlabeled data is available, run a clustering algorithm and figure out the intrinsic dimensionality which can be plugged into previous formulae. It is complicated when one has no prior knowledge and commits to a Deep Learning approach. In such cases, building a simple baselines is still going to be useful. Such a simple model can guide the data collection exercise.\n\n\nWhat about Large Language Models?\nDo we want to train custom LLMs or do we want to use them for inference?\nFor training LLMs, how much data is needed, the compute needed and the performance and emergence of the variables to study play. This area is emerging with results and counter-results. But some early works fit parametric curves to predict performance given training data (counted in terms of number of tokens) and for a given compute budget. The following regression model is considered in Training Compute-Optimal LLMs, using their notation, known as Chinchila scaling laws : \\[ L(N,D) = \\frac{A}{N^{\\alpha}} + \\frac{B}{D^{\\beta}} + E \\] where \\(L()\\) is the loss, \\(D\\) is the training dataset size in tokens, \\(N\\) is the model size, \\(A,B,E,\\alpha, \\beta\\) are unknowns to be fit from experimental data. Under this mode, \\(E\\) is the smallest loss achievable (irreducible noise), with infinite data and infinite compute. Based on large scale experiments, they fit \\[ L(N,D) = \\frac{406.4}{N^{0.34}} + \\frac{410.7}{D^{0.28}} + 1.69 \\]. It may be better to use a more standard notation and rewrite them as: \\[ E(K,N) = \\frac{A}{K^{\\alpha}} + \\frac{B}{N^{\\beta}} + \\sigma^2 \\] where \\(K\\) is the model size/complexity, \\(N\\) is the number of tokens, and \\(\\sigma^2\\) is the noise variance. See Pathways Language Model and Model Scaling from Aakanskha for more on Scaling Laws.\nSome observations w.r.t LLM scaling laws are:\n\nTraining LLMs from scratch is an extremely specialized endeavor, requiring not only deep pockets, good understanding of the LLM science but also solid (distributed) systems engineering knowledge.\nBoth data size and model size of sufficient size are needed to see emergence.\nFor instruction fine-tuning, about 1k-6k instruction pairs is considered a good start. See LIMIT: Less Is More for Instruction Tuning Across Evaluation Paradigm . More would be better in this case. Like always, quality and representativeness matter.\nFor In-Context Learning (ICL), a fewshot learning is better. But instead of prompt engineering in ad-hoc fashion, optimizing prompts in a data driven manner, using frameworks like DSPy. For example, how many and which examples to include in the fewshot ICL can be optimzed with DSPy. Fewshot is all but one of the strategies to improve the performance of LLMs. See The Prompt report for more details.\n\n\n\nTake-aways\n\nCollecting data is an iterative exercise\nPlay with several design inputs and pick a good starting point. Run several heuristics.\nTry to leverage past knowledge (datasets, models, and problem similarity) as much as possible.\nDo not collect all data in one tranche but collect often, refine the strategy and iterate.\nIncorporate practical constraints. Otherwise, data collection will not even begin.",
    "crumbs": [
      "Lectures[ML Science]",
      "07A: Scaling Laws"
    ]
  },
  {
    "objectID": "homeworks.html",
    "href": "homeworks.html",
    "title": "Homeworks",
    "section": "",
    "text": "Major Project\nTask\nNote:\nDue:\nProposal Submission:\n- 11.59PM IST, Friday, 20th Oct, 2024.\nPresentations:\n- 26th and 29th, Nov, 2024, in-class.\nFinal Submission:\n- 11.59PM IST, Friday, 29th Nov, 2024.",
    "crumbs": [
      "Homeworks"
    ]
  },
  {
    "objectID": "homeworks.html#sec-hw-major",
    "href": "homeworks.html#sec-hw-major",
    "title": "Homeworks",
    "section": "",
    "text": "This major project focuses on baking all the software engineering best practices, MLOps lifecycle and holistic ML into one code base. All the objectives of the Minor Project should be there. In addition,\n\nat dataset (aggregate) level, models are\n\nexplainable\ncalibrated\nassessed for robustness\n\nat inference time, for every instance, the following are available\n\nconformal predictions\nexplanations\ntrust scores\n\n\n\n\n\nYou can choose any problem or build on the Tabular Data provided in the class. Above must be available on major-project branch of your course private repo.\nSubmit your proposal no later than October 20th, 2024, Friday, 13.00pm IST. In the repo, create a README.md file at the root.\nIn addition to the code, you have present the project on 26th-29th November, in-class.",
    "crumbs": [
      "Homeworks"
    ]
  },
  {
    "objectID": "homeworks.html#sec-hw-minor",
    "href": "homeworks.html#sec-hw-minor",
    "title": "Homeworks",
    "section": "Minor Project",
    "text": "Minor Project\nTask\n\nThis minor (mini) project focuses on baking all the software engineering best practices and MLOps lifecycle into one code base\nCreate a branch named minor-project of your private repo.\nFor the dataset including all Tranche shared on your google drive, show that, your Models, Data, and Pipelines are\n\nProject Cards, Data Cards, Model Cards, MLOps Cards are populated and are data driven.\nCode is modular, linted, and tested\nCI/CD hooks are implemented\nModel is deployed and available via an API.\nInference can be scaled\nModel is monitored, and a new model is deployed when an opportunity or a need to redeploy is detected.\nRight to be forgotten is enabled.\n\nOne way to think about it is, it is is culmination of HWs up until now.\n\nDue by\n11.59PM IST, Friday, 25th Oct, 2024.",
    "crumbs": [
      "Homeworks"
    ]
  },
  {
    "objectID": "homeworks.html#sec-hw-06",
    "href": "homeworks.html#sec-hw-06",
    "title": "Homeworks",
    "section": "HW-06",
    "text": "HW-06\nReading\n\nRight to Erasure act of GDPR\n\nTask\n\nThis HW focuses on the ability to implement techniques to enforce the “right to erase” policy\nCreate a branch named hw06 of your private repo.\nTreat the model you have built in HW-04 as your baseline, call it Model A\nImagine the first 10 records were to be erased.\nImplement a strategy s.t those 10 records will not be used in any future predictions. Update the API, and Data and Model cards to reflect this change.\nSuggest a method to implement “right to erasure” such that, even all past predictions that were influenced by the 10 records will be re-scored, and pushed.\n\nDue by\n11.59PM IST, Friday, 18th Oct, 2024.",
    "crumbs": [
      "Homeworks"
    ]
  },
  {
    "objectID": "homeworks.html#sec-hw-05",
    "href": "homeworks.html#sec-hw-05",
    "title": "Homeworks",
    "section": "HW-05",
    "text": "HW-05\nReading\n\nmlflow Read mlflow documentation\nkedro-mlflow Read kedro-mlflow plug-in documentation\n\nTask\n\nThis HW focuses on Model Comparison and Continuous Improvement\nIt builds on HW03-04. See instructions for HW-03, HW-04.\nCreate a branch named hw05 of your private repo.\nTreat the model you have built in HW-04 as your baseline, call it Model A\nA new tranche of data is shared with you on google drive.\nAsses the performance of Model A on this new tranche data. Is the model performing well on all metrics (i.e data drift)?\nDevelop an alternate model, call it Model B. Run a Hypothesis test to see Model B is better than Model A on new tranche of data shared in your private repo.\nDeploy Model B if the performance is better than Model A on the new tranche of data. Update metadata of the API to reflect the new model version being used to make the predictions.\n\nDue by\n11.59PM IST, Friday, 20th Sep, 2024.",
    "crumbs": [
      "Homeworks"
    ]
  },
  {
    "objectID": "homeworks.html#sec-hw-04",
    "href": "homeworks.html#sec-hw-04",
    "title": "Homeworks",
    "section": "HW-04",
    "text": "HW-04\nReading\n\nmlflow Read mlflow documentation\nkedro-mlflow Read kedro-mlflow plug-in documentation\n\nTask\n\nThis HW focuses on Deployment and Monitoring.\nIt builds on HW03. See instructions for HW-03.\nCreate a branch named hw04 of your private repo.\nMake sure that\n\nDefine the request and response structure of the API. Provide API documentation.\nModel is deployed and API is available via kedro-mlflow plug-in.\nModel usage is logged\n\n\nDue by\n11.59PM IST, Friday, 13th Sep, 2024.",
    "crumbs": [
      "Homeworks"
    ]
  },
  {
    "objectID": "homeworks.html#sec-hw-03",
    "href": "homeworks.html#sec-hw-03",
    "title": "Homeworks",
    "section": "HW-03",
    "text": "HW-03\nReading\n\nEvidently: Read Evidently, an ML & Data Quality and Monitoring tool.\nCh 8 of ML System Design\nBlogs from Great Expectations\n\nTask\n\nThis HW focuses on Data-driven code structure, documentation, code hygiene, testing, and data quality assessment & reporting.\nA Google Drive folder with your github handle is shared. This contains a dataset.\nCreate a branch named hw03 of your private repo.\nSimilar to the Kedro spaceflights tutorial, write your data loading, model training, and testing, pipeline interms of nodes and pipelines.\nMake sure that\n\nAPIs are documented with quartodoc\nCode is linted and formatted (with ruff)\nNodes and Pipelines are Unit tested (with PyTest).\nData Quality and Drift Detection Test suites using Evidently are run and the reports are represented as Plotly nodes in Kedro .\nAutomated test to detect distribution drift of target variable(represented as y in the dataset) between Train and Test splits is run using Evidently. The pipeline fails if distribution drift is detected.\n\n\nDue by\n11.59PM IST, Friday, 06th Sep, 2024.",
    "crumbs": [
      "Homeworks"
    ]
  },
  {
    "objectID": "homeworks.html#sec-hw-02",
    "href": "homeworks.html#sec-hw-02",
    "title": "Homeworks",
    "section": "HW-02",
    "text": "HW-02\nReading\n\nData Cards: Purposeful and Transparent Dataset Documentation for Responsible AIpaper blog from Google Research.\n\nTask\n\nDownload the soybean tabular dataset version v1 with dataset id 42 from openML.\nMake sure that a repo is already created (as you would have done in HW-01) with repo name “your first name-ai-839” and git handle dhavala is given read and write access.\n\nCreate a new branch name hw02\nUnder notebooks folder, create a new notebook with markdown and cell tags to implement Data Cards.\nUsing pymfe, include some useful properties of the dataset in the Data Card - - Make sure that any changes in the dataset get reflected automatically in the Data Card. Specifically, if the dataset id 1023 (refers to its version v2), Data Card should reflect any changes in the new version of the data.\nThe CLeAR framework has some really nice goals for a document. A document must be 1) comparable 2) legible 3) actionable and 4) robust to achieve an aspect of AI Transparency. Here we are talking about documenting Data, Models, and AI Systems.\n\nDue by\n11.59PM IST, Friday, 23rd August, 2024.",
    "crumbs": [
      "Homeworks"
    ]
  },
  {
    "objectID": "homeworks.html#sec-hw-01",
    "href": "homeworks.html#sec-hw-01",
    "title": "Homeworks",
    "section": "HW-01",
    "text": "HW-01\nReading\n\nChip Huyen’s Lecture Notes on ML in Production. Intro here\nPhase Zero of MLOps from ml-ops.org blogs. Very useful and nicely done\nTowards CRISP-ML(Q): A Machine Learning Process Model with QA Methodology\nProject Canvas from Gokul Mohandas’s course\nAI Canvas\n\nTask\n\nPick your favorite problem. Complete the information required in the project card template\nCreate a new kedro project with repo name “your first name-ai-839”. Documentation here\nPush it to github. Give read read/write acces to git handle dhavala\nCreate a branch “hw01”. Under notebooks folder, copy project card template\nComplete the Project Card, commit and push the notebook to your github (remote) repo.\n\nDue by\n11.59PM IST, Tuesday, 13th August, 2024.",
    "crumbs": [
      "Homeworks"
    ]
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Major Project\nTask\nNote:\nDue:\nProposal Submission:\n- 11.59PM IST, Friday, 20th Oct, 2024.\nPresentations:\n- 26th and 29th, Nov, 2024, in-class.\nFinal Submission:\n- 11.59PM IST, Friday, 29th Nov, 2024.",
    "crumbs": [
      "Projects"
    ]
  },
  {
    "objectID": "projects.html#sec-hw-major",
    "href": "projects.html#sec-hw-major",
    "title": "Projects",
    "section": "",
    "text": "This major project focuses on baking all the software engineering best practices, MLOps lifecycle and holistic ML into one code base. All the objectives of the Minor Project should be there. In addition,\n\nat dataset (aggregate) level, models are\n\nexplainable\ncalibrated\nassessed for robustness\n\nat inference time, for every instance, the following are available\n\nconformal predictions\nexplanations\ntrust scores\n\n\n\n\n\nYou can choose any problem or build on the Tabular Data provided in the class. Above must be available on major-project branch of your course private repo.\nSubmit your proposal no later than October 20th, 2024, Friday, 13.00pm IST. In the repo, create a README.md file at the root.\nIn addition to the code, you have present the project on 26th-29th November, in-class.",
    "crumbs": [
      "Projects"
    ]
  },
  {
    "objectID": "projects.html#sec-hw-minor",
    "href": "projects.html#sec-hw-minor",
    "title": "Projects",
    "section": "Minor Project",
    "text": "Minor Project\nTask\n\nThis minor (mini) project focuses on baking all the software engineering best practices and MLOps lifecycle into one code base\nCreate a branch named minor-project of your private repo.\nFor the dataset including all Tranche shared on your google drive, show that, your Models, Data, and Pipelines are\n\nProject Cards, Data Cards, Model Cards, MLOps Cards are populated and are data driven.\nCode is modular, linted, and tested\nCI/CD hooks are implemented\nModel is deployed and available via an API.\nInference can be scaled\nModel is monitored, and a new model is deployed when an opportunity or a need to redeploy is detected.\nRight to be forgotten is enabled.\n\nOne way to think about it is, it is is culmination of HWs up until now.\n\nDue by\n11.59PM IST, Friday, 25th Oct, 2024.",
    "crumbs": [
      "Projects"
    ]
  },
  {
    "objectID": "tutorials.html",
    "href": "tutorials.html",
    "title": "Tutorials",
    "section": "",
    "text": "GIT",
    "crumbs": [
      "Tutorials"
    ]
  },
  {
    "objectID": "tutorials.html#sec-tut-git",
    "href": "tutorials.html#sec-tut-git",
    "title": "Tutorials",
    "section": "",
    "text": "[video] Git Tutorial for Beginners by Code-With-Mosh.\n[Paid Course] Ultimate Git by Code-with-Mosh is very good. Covers, additional topics like Collaboration, Advanced Branching strategies, etc.\n[video] Git and Github for Beginners - Crash Course from freeCodeCamp.org\n[book] Pro Git\n[web] Learn Git Branching - an interactive way to learn git branching",
    "crumbs": [
      "Tutorials"
    ]
  },
  {
    "objectID": "tutorials.html#sec-tut-git-actions",
    "href": "tutorials.html#sec-tut-git-actions",
    "title": "Tutorials",
    "section": "Git Actions",
    "text": "Git Actions\n\n[Documentation] Quick Start Github offocial documentation\n[video] Git Actions for CI/CD",
    "crumbs": [
      "Tutorials"
    ]
  },
  {
    "objectID": "tutorials.html#sec-tut-docker",
    "href": "tutorials.html#sec-tut-docker",
    "title": "Tutorials",
    "section": "Docker",
    "text": "Docker\n\n[book] The Docker Handbook Book\n[Paid Course] The Ultimate Docker Course by Code-with-Mosh is very good.\n[tutotial] Please-Contain-Yourself. Liked the title :)",
    "crumbs": [
      "Tutorials"
    ]
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks",
    "section": "",
    "text": "[pdf] Ops, Strategy Planning for AI by Soma S. Dhavala, given at DFL’s Responsible AI Impact Lab (RAIL) Fellowship program.\n[pdf] Llama.LISP: AI First Compiler Framework by Sasank Chilamkurthy, Founder & CEO Von Neumann AI\n[] Looper: An end-to-end ML platform for product decisions, Igor Markov Talk at Stanford MLSys #60",
    "crumbs": [
      "Talks"
    ]
  },
  {
    "objectID": "documentation.html",
    "href": "documentation.html",
    "title": "ML Documentation",
    "section": "",
    "text": "Model Cards\nGoogle published Model Cards for Model Reporting to improve transparency in model reporting. Idea is very similar to how information related to the nutritional content is published on the packaging. See their blog for details.",
    "crumbs": [
      "ML Documentation"
    ]
  },
  {
    "objectID": "documentation.html#business-view",
    "href": "documentation.html#business-view",
    "title": "ML Documentation",
    "section": "Business View",
    "text": "Business View\nIt focusses on the following questions:\n\nWhat is the problem being solved?\nWho is the customer?\nWhy it needs to be solved?\nHow does the solution like - a mental or a conceptual model or a mock of the product?\nWhat objectives does it achieve?\nWhat are the risks and challenges?",
    "crumbs": [
      "ML Documentation"
    ]
  },
  {
    "objectID": "documentation.html#ml-view",
    "href": "documentation.html#ml-view",
    "title": "ML Documentation",
    "section": "ML view",
    "text": "ML view\nIt is bit more in-depth focussing on the execution will have the following questions:\n\nWhat is the prediction problem?\nHow the objective will be measured?\nHow will it be tested?\nData: What kind, how and how much and what for?\nWhat is the roadmap/plan?\nWhat resources are needed - both human, compute and admin?",
    "crumbs": [
      "ML Documentation"
    ]
  },
  {
    "objectID": "documentation.html#what-is-the-solution",
    "href": "documentation.html#what-is-the-solution",
    "title": "ML Documentation",
    "section": "What is the solution",
    "text": "What is the solution\n\nSurprisingly simple. Just tag a notebook cell - who is it for?\nAnd take benefit of modern document publishing tools and workflows like Quarto, GitHub, GitHub actions",
    "crumbs": [
      "ML Documentation"
    ]
  },
  {
    "objectID": "documentation.html#core-tenets",
    "href": "documentation.html#core-tenets",
    "title": "ML Documentation",
    "section": "Core tenets",
    "text": "Core tenets\n\nOne content - many views\nData + Code + Content &gt; should drive the documentation (format, style, purpose)\nEach stakeholder’s documentation need is just a view or a content rendering problem\nPublishing documentation = Publishing code\nUse the same tools and mental models both for code and documents\nSingle source of truth for any derived document\nFix in only one place and only once.\nPhysical and mental distance between Documentation and Code should be (close to) ZERO\nSet up once and automate subsequently\nAutomate the publishing process\nNo human oversight should be necessary for process compliance\nCommit code + documentation content &gt; rendering must be automated\n\nAbove points are put in a Project Card Template which is a Jupyter notebook.",
    "crumbs": [
      "ML Documentation"
    ]
  }
]